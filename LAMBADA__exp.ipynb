{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " LAMBADA _exp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtvSmsADKyhR"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3by8TeBIMFnp"
      },
      "source": [
        "cd /content/gdrive/Shareddrives/CS726/Vaidehi/Lambada"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSY_Wz4GK8W9"
      },
      "source": [
        "!pip install nengolib\n",
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class Arguments:\n",
        "  batch_size = 20\n",
        "  cuda = False\n",
        "  seed = 1111\n",
        "  clip = 0.4\n",
        "  epochs = 1\n",
        "  data = \"/content/gdrive/Shareddrives/CS726/Vaidehi/Lambada\"\n",
        "  emsize = 500\n",
        "  log_interval = 100\n",
        "  lr = 0.4\n",
        "  nhid = 500\n",
        "  optim = \"SGD\"\n",
        "  validseqlen = 50\n",
        "  seq_len = 100\n",
        "  corpus = False #set this as true while running for the first time\n",
        "  tied = True\n",
        "\n",
        "args=Arguments()\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, restart GPU runtime\")\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHu_rlDPSwMR"
      },
      "source": [
        "def data_generator(args):\n",
        "    if os.path.exists(args.data + \"/corpus\") and not args.corpus:\n",
        "        corpus = pickle.load(open(args.data + '/corpus', 'rb'))\n",
        "    else:\n",
        "        print(\"Creating Corpus...\")\n",
        "        corpus = Corpus(args.data + \"/lambada-vocab-2.txt\", args.data)\n",
        "        pickle.dump(corpus, open(args.data + '/corpus', 'wb'))\n",
        "\n",
        "    eval_batch_size = 1\n",
        "    train_data = batchify(corpus.train, args.batch_size, args)\n",
        "    # val_data = batchify(corpus.valid, args.batch_size, args)\n",
        "    # test_data = batchify(corpus.test, args.batch_size, args)\n",
        "    val_data = [[0] * (args.seq_len-len(line)) + line for line in corpus.valid]\n",
        "    test_data = [[0] * (args.seq_len-len(line)) + line for line in corpus.test]\n",
        "    return train_data, val_data, test_data, corpus\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            # print(word)\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            # print(word)\n",
        "            # print(word not in self.word2idx)\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, dict_path, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.prep_dict(dict_path)\n",
        "        self.train = torch.LongTensor(self.tokenize(os.path.join(path, 'train-novels')))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'lambada_development_plain_text.txt'), eval=True)\n",
        "        self.test = self.tokenize(os.path.join(path, 'lambada_test_plain_text.txt'), eval=True)\n",
        "\n",
        "    def prep_dict(self, dict_path):\n",
        "        assert os.path.exists(dict_path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(dict_path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                word = line.strip().split()[0]\n",
        "                # print(word)\n",
        "                # print(\"word parinted\")\n",
        "                tokens += 1\n",
        "                # print(word)\n",
        "                self.dictionary.add_word(word)\n",
        "\n",
        "        if \"<eos>\" not in self.dictionary.word2idx:\n",
        "            self.dictionary.add_word(\"<eos>\")\n",
        "            tokens += 1\n",
        "\n",
        "        # print(\"to\" in self.dictionary.word2idx)\n",
        "\n",
        "        print(\"The dictionary captured a vocabulary of size {0}.\".format(tokens))\n",
        "\n",
        "    def tokenize(self, path, eval=False):\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        ids = []\n",
        "        token = 0\n",
        "        misses = 0\n",
        "        if not path.endswith(\".txt\"):   # it's a folder\n",
        "            for subdir in os.listdir(path):\n",
        "                for filename in os.listdir(path + \"/\" + subdir):\n",
        "                    if filename.endswith(\".txt\"):\n",
        "                        full_path = \"{0}/{1}/{2}\".format(path, subdir, filename)\n",
        "                        # Tokenize file content\n",
        "                        delta_ids, delta_token, delta_miss = self._tokenize_file(full_path, eval=eval)\n",
        "                        ids += delta_ids\n",
        "                        token += delta_token\n",
        "                        misses += delta_miss\n",
        "        else:\n",
        "            ids, token, misses = self._tokenize_file(path, eval=eval)\n",
        "\n",
        "        print(token, misses)\n",
        "        return ids\n",
        "\n",
        "    def _tokenize_file(self, path, eval=False):\n",
        "        with open(path, 'r') as f:\n",
        "            token = 0\n",
        "            ids = []\n",
        "            misses = 0\n",
        "            for line in f:\n",
        "                line_ids = []\n",
        "                words = line.strip().split() + ['<eos>']\n",
        "                # print(words)\n",
        "                # print(\"Words being printed\")\n",
        "                if eval:\n",
        "                    words = words[:-1]\n",
        "                for word in words:\n",
        "                    # These words are in the text but not vocabulary\n",
        "                    if word == \"n't\":\n",
        "                        word = \"not\"\n",
        "                    elif word == \"'s\":\n",
        "                        word = \"is\"\n",
        "                    elif word == \"'re\":\n",
        "                        word = \"are\"\n",
        "                    elif word == \"'ve\":\n",
        "                        word = \"have\"\n",
        "                    elif word == \"wo\":\n",
        "                        word = \"will\"\n",
        "                    if word not in self.dictionary.word2idx:\n",
        "                        word = re.sub(r'[^\\w\\s]', '', word)\n",
        "                    if word not in self.dictionary.word2idx:\n",
        "                        misses += 1\n",
        "                        continue\n",
        "                    line_ids.append(self.dictionary.word2idx[word])\n",
        "                    token += 1\n",
        "                if eval:\n",
        "                    ids.append(line_ids)\n",
        "                else:\n",
        "                    ids += line_ids\n",
        "        return ids, token, misses\n",
        "\n",
        "\n",
        "def batchify(data, batch_size, args):\n",
        "    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1)\n",
        "    print(data.size())\n",
        "    if args.cuda:\n",
        "        data = data.cuda()\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_batch(source, i, args, seq_len=None, evaluation=False):\n",
        "    seq_len = min(seq_len if seq_len else args.seq_len, source.size(1) - 1 - i)\n",
        "    data = Variable(source[:, i:i+seq_len], volatile=evaluation)\n",
        "    target = Variable(source[:, i+1:i+2])  # CAUTION: This is un-flattened!\n",
        "    return data, target\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndXYw3TlLRUo"
      },
      "source": [
        "train_data, val_data, test_data, corpus = data_generator(args)\n",
        "\n",
        "n_words = len(corpus.dictionary)\n",
        "print(\"Total # of words: {0}\".format(n_words))\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFIt8DDGLb_L"
      },
      "source": [
        "class LMUTagger(nn.Module):\n",
        "    def __init__(self, units, order, theta, vocab_size, embedding_dim, tagset_size):\n",
        "        super(LMUTagger, self).__init__()\n",
        "\n",
        "        self.theta = theta\n",
        "        self.units = units\n",
        "        self.order = order\n",
        "\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \n",
        "        self.lmu_cell = LMUCell(input_size=embedding_dim, hidden_size=units, memory_size=order, theta=theta, nonlinearity='sigmoid', A_learnable = True, B_learnable = True)\n",
        "\n",
        "        # uncomment the following line to use ASSVMU\n",
        "        # self.lmu_cell = ASSVMU(input_size = embedding_dim, hidden_size = units, memory_size = order, theta = theta, discretizer = 'zoh',nonlinearity='sigmoid', A_learnable = False, B_learnable = False, activate=False)\n",
        "\n",
        "        # uncomment the following line to use BMU\n",
        "        # self.lmu_cell = BMU(input_size = embedding_dim, hidden_size = units, memory_size = order, theta = theta, matrix_type='pb', discretizer = 'zoh',nonlinearity='sigmoid', A_learnable = False, B_learnable = False)\n",
        "\n",
        "\n",
        "        self.dense = nn.Linear(\n",
        "            in_features=units,\n",
        "            out_features=tagset_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs is of shape (batch_size, n_steps, 1)\n",
        "\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "\n",
        "        h = torch.zeros(1, self.units)\n",
        "        c = torch.zeros(1, self.order)\n",
        "\n",
        "        if args.cuda:\n",
        "          \n",
        "          h = h.cuda()\n",
        "          c = c.cuda()\n",
        "          embeds = embeds.cuda()\n",
        "\n",
        "        \n",
        "        for i in range(embeds.shape[1]):            \n",
        "            h, c = self.lmu_cell(embeds[:, i, :], (h, c))\n",
        "           \n",
        "        if inputs.is_cuda:\n",
        "          h = h.cuda()\n",
        "        ###\n",
        "\n",
        "        # make a prediction based on the final hidden state of the LMU\n",
        "        return self.dense(h)\n",
        "\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # print(embeds.shape)\n",
        "        lstm_out, _ = self.lstm(embeds.view(embeds.shape[1], -1, 500))\n",
        "        # print(lstm_out.shape)\n",
        "        tag_space = self.hidden2tag(lstm_out[99,:,:].view(-1,1,150))\n",
        "        # print(\"Tag space\")\n",
        "        # print(tag_space)\n",
        "        # print(tag_space.contiguous())\n",
        "        return tag_space\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DiiTzxcLiXP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nengolib.signal import Identity, cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LMUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, memory_size, theta, nonlinearity='sigmoid', A_learnable = True, B_learnable = True):\n",
        "        super(LMUCell, self).__init__()\n",
        "        \n",
        "        ### SIZE\n",
        "        self.k = input_size\n",
        "        self.n = hidden_size\n",
        "        self.d = memory_size\n",
        "\n",
        "        ### PARAMETERS\n",
        "        self.Wx = nn.Parameter(torch.Tensor(self.n,self.k))\n",
        "        self.Wh = nn.Parameter(torch.Tensor(self.n,self.n))\n",
        "        self.Wm = nn.Parameter(torch.Tensor(self.n,self.d))\n",
        "        self.ex = nn.Parameter(torch.Tensor(1,self.k))\n",
        "        self.eh = nn.Parameter(torch.Tensor(1,self.n))\n",
        "        self.em = nn.Parameter(torch.Tensor(1,self.d))\n",
        "\n",
        "        ### A,B MATRIX ----- FIX??\n",
        "        realizer = Identity()\n",
        "        self._realizer_result = realizer(LegendreDelay(theta=theta, order=self.d))\n",
        "        self._ss = cont2discrete(self._realizer_result.realization, dt=1., method='zoh')\n",
        "        self._A = self._ss.A\n",
        "        self._B = self._ss.B\n",
        "        '''\n",
        "        Q = np.arange(order, dtype=np.float64)\n",
        "        R = (2 * Q + 1)[:, None] / theta\n",
        "        j, i = np.meshgrid(Q, Q)\n",
        "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "        B = (-1.0) ** Q[:, None] * R\n",
        "        C = np.ones((1, order))\n",
        "        D = np.zeros((1,))\n",
        "        self._A, self._B, _, _, _ = cont2discrete((A, B, C, D), dt=1.0, method=\"zoh\")\n",
        "        '''\n",
        "        self.AT = torch.Tensor(self._A)\n",
        "        self.BT = torch.Tensor(self._B)\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "        ### Changes\n",
        "        # if args.cuda:\n",
        "        #   self.AT = self.AT.cuda()\n",
        "        #   self.BT = self.BT.cuda()\n",
        "        ###\n",
        "\n",
        "        ### NON-LINEARITY\n",
        "        self.nl = nonlinearity\n",
        "        if self.nl == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif self.nl == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            self.act = nn.ReLU()\n",
        "\n",
        "        ### INITIALIZATION\n",
        "        torch.nn.init.xavier_normal_(self.Wm)    ##### FIGURE THIS OUT!!\n",
        "        torch.nn.init.xavier_normal_(self.Wx)\n",
        "        torch.nn.init.xavier_normal_(self.Wh)\n",
        "        torch.nn.init.zeros_(self.em)\n",
        "        torch.nn.init.uniform_(self.ex, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        torch.nn.init.uniform_(self.eh, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        \n",
        "    def forward(self,x,hm):\n",
        "        '''\n",
        "        x shape: (batch_size, input_size) \n",
        "        h shape: (batch_size, hidden_size)\n",
        "        m shape: (batch_size, memory_size) \n",
        "        '''\n",
        "\n",
        "        h,m = hm\n",
        "        # if args.cuda:\n",
        "        #   h.cuda()\n",
        "        #   m.cuda()\n",
        "        u = F.linear(x,self.ex)+F.linear(h,self.eh)+F.linear(m,self.em)\n",
        "        # if args.cuda:\n",
        "        #   u.cuda()\n",
        "        ### Changes:\n",
        "        # print(self.AT.is_cuda, self.BT.is_cuda) prints False, False\n",
        "        # becaue torch.tensor is not part of model and hence model.cuda does not transform it to cuda\n",
        "        ####\n",
        "        new_m = F.linear(m,self.AT) + F.linear(u,self.BT)\n",
        "        # if args.cuda:\n",
        "        #   new_m.cuda()\n",
        "        new_h = self.act(F.linear(x,self.Wx)+F.linear(h,self.Wh)+F.linear(new_m,self.Wm))\n",
        "        # if args.cuda:\n",
        "        #   new_h.cuda()\n",
        "\n",
        "        return new_h,new_m\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nengolib.signal import Identity, cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "import numpy as np\n",
        "\n",
        "class ASSVMU(nn.Module):\n",
        "\n",
        "    # ASSVMU(input_size, hidden_size, memory_size, theta, discretizer = 'zoh',nonlinearity='sigmoid', \n",
        "    #                     A_learnable = False, B_learnable = False, activate=False)\n",
        "    # # '''\n",
        "    # Co-invented by Abhishek, Syomantak, Siddharth, Vaidehi, Mithilesh\n",
        "    # ASSVM + MU = ASSVMU\n",
        "    # '''\n",
        "    def __init__(self, input_size, hidden_size, memory_size, theta, discretizer = 'zoh',nonlinearity='sigmoid', \n",
        "                        A_learnable = False, B_learnable = False, activate=False):\n",
        "        super(ASSVMU, self).__init__()\n",
        "        \n",
        "        ### SIZE\n",
        "        self.k = input_size\n",
        "        self.n = hidden_size\n",
        "        self.d = memory_size\n",
        "\n",
        "        ###\n",
        "        # self.include_both = include_both\n",
        "        \n",
        "\n",
        "        ### PARAMETERS\n",
        "        self.Wx = nn.Parameter(torch.Tensor(self.n,self.k))\n",
        "        self.Wh = nn.Parameter(torch.Tensor(self.n,self.n))\n",
        "        self.Wm = nn.Parameter(torch.Tensor(self.n,self.d))\n",
        "        self.ex = nn.Parameter(torch.Tensor(1,self.k))\n",
        "        self.eh = nn.Parameter(torch.Tensor(1,self.n))\n",
        "        self.em = nn.Parameter(torch.Tensor(1,self.d))\n",
        "\n",
        "        ### A,B MATRIX ----- FIX??\n",
        "        order=self.d\n",
        "        Q = np.arange(order, dtype=np.float64)\n",
        "        R = (2 * Q + 1)[:, None] / theta\n",
        "        j, i = np.meshgrid(Q, Q)\n",
        "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "        B = (-1.0) ** Q[:, None] * R\n",
        "        C = np.ones((1, order))\n",
        "        D = np.zeros((1,))\n",
        "        self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "        self._A = self._ss.A\n",
        "        self._B = self._ss.B\n",
        "\n",
        "        ### NON-LINEARITY\n",
        "        self.nl = nonlinearity\n",
        "        if self.nl == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif self.nl == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            self.act = nn.ReLU()\n",
        "\n",
        "        ### NN\n",
        "        self.fc = nn.Linear(self.n,self.n)\n",
        "\n",
        "        if activate:\n",
        "            self.nn_act = self.act\n",
        "        else:\n",
        "            self.nn_act = nn.LeakyReLU(1.0) #Identity Function\n",
        "\n",
        "        ### INITIALIZATION\n",
        "        torch.nn.init.xavier_normal_(self.Wm)    ##### FIGURE THIS OUT!!\n",
        "        torch.nn.init.xavier_normal_(self.Wx)\n",
        "        torch.nn.init.xavier_normal_(self.Wh)\n",
        "        torch.nn.init.zeros_(self.em)\n",
        "        torch.nn.init.uniform_(self.ex, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        torch.nn.init.uniform_(self.eh, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "\n",
        "\n",
        "        #### TRIAL\n",
        "        self.register_buffer('AT', torch.Tensor(self._A))\n",
        "        self.register_buffer('BT', torch.Tensor(self._B))\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "\n",
        "    def forward(self,x,hm):\n",
        "        '''\n",
        "        x shape: (batch_size, input_size) \n",
        "        h shape: (batch_size, hidden_size)\n",
        "        m shape: (batch_size, memory_size) \n",
        "        '''\n",
        "\n",
        "        h,m = hm \n",
        "        u = F.linear(x,self.ex)+F.linear(h,self.eh)+F.linear(m,self.em)\n",
        "        new_m = F.linear(m,self.AT) + F.linear(u,self.BT)\n",
        "        new_h = self.act(F.linear(x,self.Wx)+F.linear(h,self.Wh)+F.linear(new_m,self.Wm))\n",
        "        new_h = self.nn_act(self.fc(new_h))\n",
        "        return new_h,new_m\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nengolib.signal import Identity,cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "import numpy as np\n",
        "from scipy.special import comb\n",
        "\n",
        "\n",
        "class BMU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, memory_size, theta, matrix_type='pb',discretizer = 'zoh',nonlinearity='sigmoid', A_learnable = False, B_learnable = False):\n",
        "        super(BMU, self).__init__()\n",
        "\n",
        "        ### SIZE\n",
        "        self.k = input_size\n",
        "        self.n = hidden_size\n",
        "        self.d = memory_size\n",
        "\n",
        "        ### PARAMETERS\n",
        "        self.Wx = nn.Parameter(torch.Tensor(self.n,self.k))\n",
        "        self.Wh = nn.Parameter(torch.Tensor(self.n,self.n))\n",
        "        self.Wm = nn.Parameter(torch.Tensor(self.n,self.d))\n",
        "        self.ex = nn.Parameter(torch.Tensor(1,self.k))\n",
        "        self.eh = nn.Parameter(torch.Tensor(1,self.n))\n",
        "        self.em = nn.Parameter(torch.Tensor(1,self.d))\n",
        "\n",
        "        ### A,B MATRIX ----- FIX??\n",
        "        '''\n",
        "        realizer = Identity()\n",
        "        self._realizer_result = realizer(LegendreDelay(theta=theta, order=self.d))\n",
        "        self._ss = cont2discrete(self._realizer_result.realization, dt=1., method=discretizer)\n",
        "        self._A = self._ss.A\n",
        "        self._B = self._ss.B\n",
        "        '''\n",
        "\n",
        "        if matrix_type=='pl':   #For Legendre Memory Unit\n",
        "            order=self.d\n",
        "            Q = np.arange(order, dtype=np.float64)\n",
        "            R = (2 * Q + 1)[:, None] / theta\n",
        "            j, i = np.meshgrid(Q, Q)\n",
        "            A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "            B = (-1.0) ** Q[:, None] * R\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "        elif matrix_type=='p':  #For Pade Memory Unit\n",
        "            order=self.d\n",
        "            Q=np.arange(order,dtype=np.float64)\n",
        "            V=(order+Q+1)*(order-Q)/(Q+1)/theta\n",
        "            A=np.zeros([order,order],dtype=np.float64)\n",
        "            B=np.zeros([order,1],dtype=np.float64)\n",
        "            A[0,:]=-V[0]\n",
        "            A[1:order,0:order-1]=np.diag(V[1:order])\n",
        "            B[0]=V[0]\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "        elif matrix_type=='pb':  #For Bernstein Memory Unit\n",
        "            order=self.d\n",
        "            Q = np.arange(order, dtype=np.float64)\n",
        "            R = (2 * Q + 1)[:, None] / theta\n",
        "            j, i = np.meshgrid(Q, Q)\n",
        "            A_leg = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "            B_leg = (-1.0) ** Q[:, None] * R\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            M=np.zeros([order,order],dtype=np.float64)\n",
        "            M_inv=np.zeros([order,order],dtype=np.float64)\n",
        "            n=order-1 #degree of polynomial\n",
        "            for j in range(0,n+1):\n",
        "              for k in range(0,n+1):\n",
        "                ll=max(0,j+k-n)\n",
        "                ul=min(j,k)+1\n",
        "                sum=0.0\n",
        "                for i in range(ll,ul):\n",
        "                  sum=sum+((-1.0)**(k+i))*(comb(k,i)**2)*comb(n-k,j-i)\n",
        "                M[j,k]=sum/comb(n,j)\n",
        "\n",
        "                sum=0.0\n",
        "                for i in range(0,j+1):\n",
        "                  sum=sum+(-1.0)**(j+i)*comb(j,i)**2/comb(n+j,k+i)\n",
        "                M_inv[j,k]=(2*j+1)/(n+j+1)*comb(n,k)*sum\n",
        "\n",
        "            M=10*np.tanh(M/10)\n",
        "            M_inv=10*np.tanh(M_inv/10)\n",
        "\n",
        "            A_1=np.matmul(M,A_leg)\n",
        "            A=np.matmul(A_1,M_inv)\n",
        "            B=np.matmul(M,B_leg)\n",
        "\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "        '''\n",
        "        self.AT = torch.Tensor(self._A)\n",
        "        self.BT = torch.Tensor(self._B)\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "        '''\n",
        "        ### NON-LINEARITY\n",
        "        self.nl = nonlinearity\n",
        "        if self.nl == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif self.nl == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            self.act = nn.ReLU()\n",
        "\n",
        "        ### INITIALIZATION\n",
        "        torch.nn.init.xavier_normal_(self.Wm)    ##### FIGURE THIS OUT!!\n",
        "        torch.nn.init.xavier_normal_(self.Wx)\n",
        "        torch.nn.init.xavier_normal_(self.Wh)\n",
        "        torch.nn.init.zeros_(self.em)\n",
        "        torch.nn.init.uniform_(self.ex, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        torch.nn.init.uniform_(self.eh, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "\n",
        "\n",
        "        #### TRIAL\n",
        "        self.register_buffer('AT', torch.Tensor(self._A))\n",
        "        self.register_buffer('BT', torch.Tensor(self._B))\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "\n",
        "\n",
        "    def forward(self,x,hm):\n",
        "        '''\n",
        "        x shape: (batch_size, input_size)\n",
        "        h shape: (batch_size, hidden_size)\n",
        "        m shape: (batch_size, memory_size)\n",
        "        '''\n",
        "\n",
        "        h,m = hm\n",
        "        u = F.linear(x,self.ex)+F.linear(h,self.eh)+F.linear(m,self.em)\n",
        "        new_m = F.linear(m,self.AT) + F.linear(u,self.BT)\n",
        "        new_h = self.act(F.linear(x,self.Wx)+F.linear(h,self.Wh)+F.linear(new_m,self.Wm))\n",
        "\n",
        "        return new_h,new_m\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT9096GxL-Ej"
      },
      "source": [
        "model=LMUTagger(order=40, theta=35**2,embedding_dim=500,units=150,vocab_size=112742,tagset_size=112742)\n",
        "# model=LSTMTagger(embedding_dim=500,hidden_dim=150,vocab_size=112742,tagset_size=112742)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLsaSZ3QMBay"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 4\n",
        "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    processed_data_size = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(data_source)):\n",
        "            data, targets = torch.LongTensor(data_source[i]).view(1, -1), torch.LongTensor([data_source[i][-1]]).view(1, -1)\n",
        "            data, targets = Variable(data), Variable(targets)\n",
        "            \n",
        "            if args.cuda:\n",
        "                data, targets = data.cuda(), targets.cuda()\n",
        "            output = model(data)\n",
        "            final_output = output[:, :].contiguous().view(-1, n_words)\n",
        "            final_target = targets[:, -1].contiguous().view(-1)\n",
        "            loss = criterion(final_output, final_target)\n",
        "            total_loss += loss.data\n",
        "            processed_data_size += 1\n",
        "        return total_loss.item() / processed_data_size\n",
        "\n",
        "\n",
        "def train():\n",
        "    global train_data\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, i in enumerate(range(147000, (train_data.size(1) - 1), args.validseqlen)):\n",
        "      # if batch_idx > 128900:\n",
        "        if i + args.seq_len - args.validseqlen >= train_data.size(1) - 1:\n",
        "            continue\n",
        "        data, targets = get_batch(train_data, i, args)\n",
        "        if args.cuda:\n",
        "            data, targets = data.cuda(), targets.cuda()\n",
        "       \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        eff_history = args.seq_len - args.validseqlen\n",
        "        if eff_history < 0:\n",
        "            raise ValueError(\"Valid sequence length must be smaller than sequence length!\")\n",
        "        final_target = targets[:, :].contiguous().view(-1)\n",
        "\n",
        "        #when using LSTMTagger uncomment the following line\n",
        "        # output = output[:, :].contiguous().view(-1, n_words)\n",
        "        \n",
        "        loss = criterion(output, final_target)\n",
        "        loss.backward()\n",
        "        if args.clip > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.5f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch_idx, train_data.size(1) // args.validseqlen, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            \n",
        "            print('Save model!\\n')\n",
        "            torch.save(model.state_dict(), \"weights/model_weights_LMU_learnable\")\n",
        "            total_loss = 0\n",
        "            reg_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWEQNDW5LXSU"
      },
      "source": [
        "if(True):    \n",
        "    best_vloss = 1e8\n",
        "    try:\n",
        "        all_vloss = []\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "            epoch_start_time = time.time()\n",
        "            train()\n",
        "            val_loss = evaluate(val_data)\n",
        "            test_loss = evaluate(test_data)\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                    'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                               val_loss, math.exp(val_loss)))\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
        "                  'test ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                            test_loss, math.exp(test_loss)))\n",
        "            print('-' * 89)\n",
        "            # Save the model if the validation loss is the best we've seen so far.\n",
        "\n",
        "            if val_loss < best_vloss:\n",
        "                \n",
        "                torch.save(model.state_dict(), \"model_LMU_learnable\")\n",
        "                print('Save model!\\n')\n",
        "\n",
        "                best_vloss = val_loss\n",
        "            if epoch > 5 and val_loss >= max(all_vloss[-5:]):\n",
        "                lr = lr / 10.\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "            all_vloss.append(val_loss)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "   \n",
        "    model.load_state_dict(torch.load(\"weights/model_weights_LMU_learnable\"), strict=False)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_loss = evaluate(test_data)\n",
        "    print('=' * 89)\n",
        "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "        test_loss, math.exp(test_loss)))\n",
        "    print('=' * 89)"
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}