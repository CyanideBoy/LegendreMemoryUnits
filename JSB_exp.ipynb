{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JSB_exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyL9IATEAsQm"
      },
      "source": [
        "import _pickle as cPickle\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torch import LongTensor\n",
        "from torch.nn import Embedding, LSTM\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xVY0_r6sRn0"
      },
      "source": [
        "with open('JSB Chorales.pickle', 'rb') as p:\n",
        "    data = cPickle.load(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU3fNIGMHL7J"
      },
      "source": [
        "def convert_binary(piano_keys):\n",
        "  piano_binary = torch.zeros(88)\n",
        "  \n",
        "  for key in piano_keys:\n",
        "    piano_binary[key-21] = 1\n",
        "  \n",
        "  return piano_binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bh0hupYQzF7"
      },
      "source": [
        "class JSB_Chorales(Dataset):\n",
        "    def __init__(self, pickle_file_path='filepath', type_data='string'):\n",
        "      \n",
        "      with open(pickle_file_path, 'rb') as p:\n",
        "        data = cPickle.load(p, encoding=\"latin1\")\n",
        "\n",
        "      self.sequences = data[type_data]\n",
        "      self.lengths = [len(sentence) for sentence in data[type_data]]\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      longest_sent = max(self.lengths)\n",
        "      seq = self.sequences[index]\n",
        "      inp = torch.zeros(longest_sent,88)\n",
        "      i = 0\n",
        "      for time in seq:\n",
        "        inp[i,:] = convert_binary(time)\n",
        "        i = i + 1\n",
        "      \n",
        "      label = torch.zeros(longest_sent,88)\n",
        "      label[:longest_sent-1,:] = inp[1:,:]\n",
        "\n",
        "      return inp, label, self.lengths[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIm4wWw1SfmS"
      },
      "source": [
        "train_dataset = JSB_Chorales(pickle_file_path='JSB Chorales.pickle',type_data='train')\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size = len(data['train']), \n",
        "                                            shuffle=True, \n",
        "                                            num_workers=1)\n",
        "\n",
        "valid_dataset = JSB_Chorales(pickle_file_path='JSB Chorales.pickle',type_data='valid')\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                               batch_size = len(data['valid']), \n",
        "                                            shuffle=False, \n",
        "                                            num_workers=1)\n",
        "\n",
        "test_dataset = JSB_Chorales(pickle_file_path='JSB Chorales.pickle',type_data='test')\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                               batch_size = len(data['test']), \n",
        "                                            shuffle=False, \n",
        "                                            num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuY0J1FqjBfE"
      },
      "source": [
        "# **Using LSTMCell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRNCJLks9Q_x"
      },
      "source": [
        "lstm = nn.LSTMCell(88, 400)\n",
        "linear = nn.Linear(400,88)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnCG_vsF9vbK"
      },
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer_lstm = optim.Adam(lstm.parameters())\n",
        "optimizer_linear = optim.Adam(linear.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ZrjQqf9yJZ"
      },
      "source": [
        "def train_model(num_epochs):\n",
        "\n",
        "  Best_loss = 1000.00  #set to a very large value\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "    lstm.train()\n",
        "    linear.train()\n",
        "    running_loss_train = 0.0\n",
        "    running_loss_valid = 0.0\n",
        "\n",
        "    for inputs, labels, lengths in train_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "\n",
        "      inputs = inputs.permute(1,0,2)\n",
        "      labels = labels.permute(1,0,2)\n",
        "      optimizer_lstm.zero_grad()\n",
        "      optimizer_linear.zero_grad()\n",
        "      \n",
        "      hx = torch.randn(229, 400) # (batch, hidden_size)\n",
        "      cx = torch.randn(229, 400)\n",
        "      output_lstm = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lstm(inputs[i], (hx, cx))\n",
        "          output_lstm.append(hx)\n",
        "\n",
        "      output_lstm = torch.stack(output_lstm, dim=0)\n",
        "      output = linear(output_lstm)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer_lstm.step()\n",
        "      optimizer_linear.step()\n",
        "\n",
        "      running_loss_train += loss.item() * inputs.size(0)\n",
        "      epoch_loss_train = running_loss_train / len(train_dataset)\n",
        "      \n",
        "      loss_values_train.append(epoch_loss_train)\n",
        "    print(f'Train Loss: {epoch_loss_train:.8f}')\n",
        "\n",
        "    #Validation starts here (for each epoch)\n",
        "    lstm.eval()\n",
        "    linear.eval()\n",
        "\n",
        "\n",
        "    for inputs, labels, lengths in valid_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "      inputs = inputs.reshape(144,76,88)  #(time_steps, batch, hidden_size)\n",
        "      labels = labels.reshape(144,76,88)\n",
        "      \n",
        "      hx = torch.randn(76, 400) # (batch, hidden_size)\n",
        "      cx = torch.randn(76, 400)\n",
        "      output_lstm = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lstm(inputs[i], (hx, cx))\n",
        "          output_lstm.append(hx)\n",
        "\n",
        "      output_lstm = torch.stack(output_lstm, dim=0)\n",
        "      output = linear(output_lstm)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      running_loss_valid += loss.item() * inputs.size(0)\n",
        "      epoch_loss_valid = running_loss_valid / len(valid_dataset)\n",
        "      \n",
        "      loss_values_valid.append(epoch_loss_valid)\n",
        "      break\n",
        "    print(f'Valid Loss: {epoch_loss_valid:.8f}')\n",
        "\n",
        "    if(epoch_loss_valid<Best_loss):\n",
        "      Best_loss = epoch_loss_valid\n",
        "      torch.save(lstm.state_dict(), 'lstm_best'+'.pt')\n",
        "      torch.save(linear.state_dict(), 'linear_lstm_best'+'.pt')\n",
        "\n",
        "\n",
        "   \n",
        "loss_values_train = []\n",
        "loss_values_valid = []   \n",
        "model_ft = train_model(num_epochs=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTbu7q0K9w2C"
      },
      "source": [
        "def test_model():\n",
        "    lstm.eval()\n",
        "    linear.eval()\n",
        "\n",
        "    for inputs, labels, lengths in test_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "      inputs = inputs.reshape(160,77,88)  #(time_steps, batch, hidden_size)\n",
        "      labels = labels.reshape(160,77,88)\n",
        "      \n",
        "      hx = torch.zeros(77, 400) # (batch, hidden_size)\n",
        "      cx = torch.zeros(77, 400)\n",
        "      output_lstm = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lstm(inputs[i], (hx, cx))\n",
        "          output_lstm.append(hx)\n",
        "\n",
        "      output_lstm = torch.stack(output_lstm, dim=0)\n",
        "      output = linear(output_lstm)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      running_loss_test = loss.item() * inputs.size(0)\n",
        "      epoch_loss_test = running_loss_test / len(test_dataset)\n",
        "    print(f'Test Loss: {epoch_loss_test:.8f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-W3qo1e-cW6"
      },
      "source": [
        "lstm.load_state_dict(torch.load(\"lstm_best.pt\"))\n",
        "linear.load_state_dict(torch.load(\"linear_lstm_best.pt\"))\n",
        "test_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd_PODL1J5e4"
      },
      "source": [
        "# **Using LMUCell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV9dCp_RKZJm"
      },
      "source": [
        "!pip install nengolib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQqgUoKKP2f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nengolib.signal import Identity,cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "import numpy as np\n",
        "from scipy.special import comb\n",
        "\n",
        "\n",
        "class LMU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, memory_size, theta, matrix_type='pl',discretizer = 'zoh',nonlinearity='sigmoid', A_learnable = False, B_learnable = False):\n",
        "        super(LMU, self).__init__()\n",
        "\n",
        "        ### SIZE\n",
        "        self.k = input_size\n",
        "        self.n = hidden_size\n",
        "        self.d = memory_size\n",
        "\n",
        "        ### PARAMETERS\n",
        "        self.Wx = nn.Parameter(torch.Tensor(self.n,self.k))\n",
        "        self.Wh = nn.Parameter(torch.Tensor(self.n,self.n))\n",
        "        self.Wm = nn.Parameter(torch.Tensor(self.n,self.d))\n",
        "        self.ex = nn.Parameter(torch.Tensor(1,self.k))\n",
        "        self.eh = nn.Parameter(torch.Tensor(1,self.n))\n",
        "        self.em = nn.Parameter(torch.Tensor(1,self.d))\n",
        "\n",
        "\n",
        "        if matrix_type=='pl':   #For Legendre Memory Unit\n",
        "            order=self.d\n",
        "            Q = np.arange(order, dtype=np.float64)\n",
        "            R = (2 * Q + 1)[:, None] / theta\n",
        "            j, i = np.meshgrid(Q, Q)\n",
        "            A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "            B = (-1.0) ** Q[:, None] * R\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "        elif matrix_type=='p':  #For Pade Memory Unit\n",
        "            order=self.d\n",
        "            Q=np.arange(order,dtype=np.float64)\n",
        "            V=(order+Q+1)*(order-Q)/(Q+1)/theta\n",
        "            A=np.zeros([order,order],dtype=np.float64)\n",
        "            B=np.zeros([order,1],dtype=np.float64)\n",
        "            A[0,:]=-V[0]\n",
        "            A[1:order,0:order-1]=np.diag(V[1:order])\n",
        "            B[0]=V[0]\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "        elif matrix_type=='pb':  #For Bernstein Memory Unit\n",
        "            order=self.d\n",
        "            Q = np.arange(order, dtype=np.float64)\n",
        "            R = (2 * Q + 1)[:, None] / theta\n",
        "            j, i = np.meshgrid(Q, Q)\n",
        "            A_leg = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "            B_leg = (-1.0) ** Q[:, None] * R\n",
        "            C = np.ones((1, order))\n",
        "            D = np.zeros((1,))\n",
        "            M=np.zeros([order,order],dtype=np.float64)\n",
        "            M_inv=np.zeros([order,order],dtype=np.float64)\n",
        "            n=order-1 #degree of polynomial\n",
        "            for j in range(0,n+1):\n",
        "              for k in range(0,n+1):\n",
        "                ll=max(0,j+k-n)\n",
        "                ul=min(j,k)+1\n",
        "                sum=0.0\n",
        "                for i in range(ll,ul):\n",
        "                  sum=sum+((-1.0)**(k+i))*(comb(k,i)**2)*comb(n-k,j-i)\n",
        "                M[j,k]=sum/comb(n,j)\n",
        "\n",
        "                sum=0.0\n",
        "                for i in range(0,j+1):\n",
        "                  sum=sum+(-1.0)**(j+i)*comb(j,i)**2/comb(n+j,k+i)\n",
        "                M_inv[j,k]=(2*j+1)/(n+j+1)*comb(n,k)*sum\n",
        "\n",
        "            M=10*np.tanh(M/10)\n",
        "            M_inv=10*np.tanh(M_inv/10)\n",
        "\n",
        "            A_1=np.matmul(M,A_leg)\n",
        "            A=np.matmul(A_1,M_inv)\n",
        "            B=np.matmul(M,B_leg)\n",
        "\n",
        "            self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "            self._A = self._ss.A\n",
        "            self._B = self._ss.B\n",
        "\n",
        "        ### NON-LINEARITY\n",
        "        self.nl = nonlinearity\n",
        "        if self.nl == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif self.nl == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            self.act = nn.ReLU()\n",
        "\n",
        "        ### INITIALIZATION\n",
        "        torch.nn.init.xavier_normal_(self.Wm)    ##### FIGURE THIS OUT!!\n",
        "        torch.nn.init.xavier_normal_(self.Wx)\n",
        "        torch.nn.init.xavier_normal_(self.Wh)\n",
        "        torch.nn.init.zeros_(self.em)\n",
        "        torch.nn.init.uniform_(self.ex, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        torch.nn.init.uniform_(self.eh, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "\n",
        "\n",
        "        #### TRIAL\n",
        "        self.register_buffer('AT', torch.Tensor(self._A))\n",
        "        self.register_buffer('BT', torch.Tensor(self._B))\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "\n",
        "\n",
        "    def forward(self,x,hm):\n",
        "\n",
        "        h,m = hm\n",
        "        u = F.linear(x,self.ex)+F.linear(h,self.eh)+F.linear(m,self.em)\n",
        "        new_m = F.linear(m,self.AT) + F.linear(u,self.BT)\n",
        "        new_h = self.act(F.linear(x,self.Wx)+F.linear(h,self.Wh)+F.linear(new_m,self.Wm))\n",
        "\n",
        "        return new_h,new_m\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2XT0lnfOy9X"
      },
      "source": [
        "class ASSVMU(nn.Module):\n",
        "    '''\n",
        "    Also known as Linear LMU\n",
        "    '''\n",
        "    def __init__(self, input_size, hidden_size, memory_size, theta, name='garbage', discretizer = 'zoh',nonlinearity='sigmoid', \n",
        "                        A_learnable = False, B_learnable = False, activate=False):\n",
        "        super(ASSVMU, self).__init__()\n",
        "        \n",
        "        ### SIZE\n",
        "        self.k = input_size\n",
        "        self.n = hidden_size\n",
        "        self.d = memory_size\n",
        "        \n",
        "\n",
        "        ### PARAMETERS\n",
        "        self.Wx = nn.Parameter(torch.Tensor(self.n,self.k))\n",
        "        self.Wh = nn.Parameter(torch.Tensor(self.n,self.n))\n",
        "        self.Wm = nn.Parameter(torch.Tensor(self.n,self.d))\n",
        "        self.ex = nn.Parameter(torch.Tensor(1,self.k))\n",
        "        self.eh = nn.Parameter(torch.Tensor(1,self.n))\n",
        "        self.em = nn.Parameter(torch.Tensor(1,self.d))\n",
        "\n",
        "        ### A,B MATRIX ----- FIX??\n",
        "        order=self.d\n",
        "        Q = np.arange(order, dtype=np.float64)\n",
        "        R = (2 * Q + 1)[:, None] / theta\n",
        "        j, i = np.meshgrid(Q, Q)\n",
        "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
        "        B = (-1.0) ** Q[:, None] * R\n",
        "        C = np.ones((1, order))\n",
        "        D = np.zeros((1,))\n",
        "        self._ss = cont2discrete((A, B, C, D), dt=0.01, method=discretizer)\n",
        "        self._A = self._ss.A\n",
        "        self._B = self._ss.B\n",
        "\n",
        "        ### NON-LINEARITY\n",
        "        self.nl = nonlinearity\n",
        "        if self.nl == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif self.nl == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            self.act = nn.ReLU()\n",
        "\n",
        "        ### NN\n",
        "        self.fc = nn.Linear(self.n,self.n)\n",
        "\n",
        "        if activate:\n",
        "            self.nn_act = self.act\n",
        "        else:\n",
        "            self.nn_act = nn.LeakyReLU(1.0) #Identity Function\n",
        "\n",
        "        ### INITIALIZATION\n",
        "        torch.nn.init.xavier_normal_(self.Wm)    ##### FIGURE THIS OUT!!\n",
        "        torch.nn.init.xavier_normal_(self.Wx)\n",
        "        torch.nn.init.xavier_normal_(self.Wh)\n",
        "        torch.nn.init.zeros_(self.em)\n",
        "        torch.nn.init.uniform_(self.ex, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "        torch.nn.init.uniform_(self.eh, -np.sqrt(3/self.d), np.sqrt(3/self.d))\n",
        "\n",
        "\n",
        "        #### TRIAL\n",
        "        self.register_buffer('AT', torch.Tensor(self._A))\n",
        "        self.register_buffer('BT', torch.Tensor(self._B))\n",
        "        if A_learnable:\n",
        "            self.AT = nn.Parameter(self.AT)\n",
        "        if B_learnable:\n",
        "            self.BT = nn.Parameter(self.BT)\n",
        "\n",
        "    def forward(self,x,hm):\n",
        "\n",
        "        h,m = hm \n",
        "        u = F.linear(x,self.ex)+F.linear(h,self.eh)+F.linear(m,self.em)\n",
        "        new_m = F.linear(m,self.AT) + F.linear(u,self.BT)\n",
        "        new_h = self.act(F.linear(x,self.Wx)+F.linear(h,self.Wh)+F.linear(new_m,self.Wm))\n",
        "        new_h = self.nn_act(self.fc(new_h))\n",
        "        return new_h,new_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIRa3T0WjFgd"
      },
      "source": [
        "lmu = ASSVMU(input_size=88,hidden_size=400,memory_size=44,theta=5)\n",
        "linear = nn.Linear(400,88)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuTDn1pSKJ92"
      },
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer_lmu = optim.Adam(lmu.parameters())\n",
        "optimizer_linear = optim.Adam(linear.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4zkjTEiKMxW"
      },
      "source": [
        "def train_model(num_epochs): \n",
        "\n",
        "  Best_loss = 1000.00  #set to a very large value\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "    lmu.train()\n",
        "    running_loss_train = 0.0\n",
        "    running_loss_valid = 0.0\n",
        "\n",
        "    for inputs, labels, lengths in train_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "\n",
        "      inputs = inputs.permute(1,0,2)\n",
        "      labels = labels.permute(1,0,2)\n",
        "\n",
        "      optimizer_lmu.zero_grad()\n",
        "      optimizer_linear.zero_grad()\n",
        "      \n",
        "      hx = torch.zeros(229, 400) # (batch, hidden_size)\n",
        "      cx = torch.zeros(229, 44)\n",
        "      output_lmu = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lmu(inputs[i], (hx,cx))\n",
        "          output_lmu.append(hx)\n",
        "\n",
        "      output_lmu = torch.stack(output_lmu, dim=0)\n",
        "      output = linear(output_lmu)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer_lmu.step()\n",
        "      optimizer_linear.step()\n",
        "      running_loss_train += loss.item() * inputs.size(0)\n",
        "      epoch_loss_train = running_loss_train / len(train_dataset)\n",
        "\n",
        "      lmu_loss_values_train.append(epoch_loss_train)\n",
        "    print(f'Train Loss: {epoch_loss_train:.8f}')\n",
        "\n",
        "    lmu.eval()\n",
        "    linear.eval()\n",
        "\n",
        "\n",
        "    for inputs, labels, lengths in valid_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "\n",
        "      inputs = inputs.permute(1,0,2)\n",
        "      labels = labels.permute(1,0,2)\n",
        "      \n",
        "      hx = torch.zeros(76, 400) # (batch, hidden_size)\n",
        "      cx = torch.zeros(76, 44)\n",
        "      output_lmu = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lmu(inputs[i], (hx,cx))\n",
        "          output_lmu.append(hx)\n",
        "\n",
        "      output_lmu = torch.stack(output_lmu, dim=0)\n",
        "      output = linear(output_lmu)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      running_loss_valid += loss.item() * inputs.size(0)\n",
        "      epoch_loss_valid = running_loss_valid / len(valid_dataset)\n",
        "      lmu_loss_values_valid.append(epoch_loss_valid)\n",
        "      break\n",
        "    print(f'Valid Loss: {epoch_loss_valid:.8f}')\n",
        "    \n",
        "    if(epoch_loss_valid<Best_loss):\n",
        "      Best_loss = epoch_loss_valid\n",
        "      torch.save(lmu.state_dict(), 'lmu_best'+'.pt')\n",
        "      torch.save(linear.state_dict(), 'linear_lmu_best'+'.pt')\t\t\n",
        "  \n",
        "lmu_loss_values_train = []\n",
        "lmu_loss_values_valid = []   \n",
        "model_ft = train_model(num_epochs=1000)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY4kYg0OAkUC"
      },
      "source": [
        "def test_model():\n",
        "    lmu.eval()\n",
        "    linear.eval()\n",
        "\n",
        "    for inputs, labels, lengths in test_dataloader:\n",
        "      \n",
        "      loss = 0.0 \n",
        "\n",
        "      inputs = inputs.reshape(160,77,88)  #(time_steps, batch, hidden_size)\n",
        "      labels = labels.reshape(160,77,88)\n",
        "      \n",
        "      hx = torch.zeros(77, 400) # (batch, hidden_size)\n",
        "      cx = torch.zeros(77, 44)\n",
        "      output_lmu = []\n",
        "      \n",
        "      for i in range(inputs.size()[0]):\n",
        "          hx, cx = lmu(inputs[i], (hx,cx))\n",
        "          output_lmu.append(hx)\n",
        "\n",
        "      output_lmu = torch.stack(output_lmu, dim=0)\n",
        "      output = linear(output_lmu)\n",
        "            \n",
        "      for i in range(inputs.size()[1]):\n",
        "        x = lengths[i]\n",
        "        loss += loss_function(output[:x,i,:],labels[:x,i,:])\n",
        "\n",
        "      running_loss_test = loss.item() * inputs.size(0)\n",
        "      epoch_loss_test = running_loss_test / len(test_dataset)\n",
        "      break\n",
        "    print(f'Test Loss: {epoch_loss_test:.8f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zl-0YXVBL5f"
      },
      "source": [
        "lmu = ASSVMU(input_size=88,hidden_size=400,memory_size=44,theta=5)\n",
        "linear = nn.Linear(400,88)\n",
        "\n",
        "lmu.load_state_dict(torch.load(\"lmu_best.pt\"))\n",
        "linear.load_state_dict(torch.load(\"linear_lmu_best.pt\"))\n",
        "\n",
        "test_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}